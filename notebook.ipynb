{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyO9B/TYGUc+26D4o1lv2uqS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dataeducator/capstone/blob/main/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Project Submission:Capstone\n",
        "(Capstone)\n",
        "\n",
        "- Student Name: Tenicka Norwood\n",
        "- Program Pace: self-paced\n",
        "- Scheduled Project Review Time: Tuesday, September 19, 2023, 12 pm\n",
        "- Instructor name: Morgan Jones\n",
        "- Blog post Url: https://medium.com/mlearning-ai/fueling-student-success-1723abd2991b"
      ],
      "metadata": {
        "id": "cdn2T52V4tmC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project I will be using __CRISP-DM__ process which has six phases:\n",
        "\n",
        "* Business Understanding __&#8594;__ Understanding the project objectives, requirements, and constraints from a business perspective.\n",
        "\n",
        "* Data Understanding __&#8594;__ Exploring and assessing the available data, its quality, structure, and initial insights.\n",
        "\n",
        "* Data Preparation __&#8594;__ Cleaning, transforming, and preparing the data to be used for modeling, including handling missing values and outliers.\n",
        "\n",
        "* Modeling __&#8594;__ Selecting and applying appropriate machine learning algorithms or techniques to build predictive or descriptive models.\n",
        "\n",
        "* Evaluation __&#8594;__ Assessing the performance of the models and determining their suitability for solving the business problem.\n",
        "\n",
        "* Deployment __&#8594;__ Integrating the chosen model into the business environment, making it accessible for end-users."
      ],
      "metadata": {
        "id": "tg7jRLIIQRaJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Business Understanding\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0LQdzJxA5BS5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### __Disclaimer:__\n",
        "This Jupyter notebook and its contents are __intended solely for educational purposes__. The included business case and the results of the deep learning models should not be interpreted as medical advice, and have not received endorsement or approval from any professional or medical organization.\n",
        "\n",
        "The models and outcomes presented here are for illustrative purposes __only__. Users should __not__ use these models or their outcomes for making real-world decisions without consulting appropriate domain experts and medical professionals. Any actions taken based on the information in this notebook are at the user's own risk.\n",
        "The author and contributors of this notebook disclaim any liability for the accuracy, completeness, or efficacy of the information provided.\n",
        "\n",
        "#### __Overview:__\n",
        "\n",
        "#### __Objectives and Goals:__\n",
        "\n",
        "#### __Problem Statement:__\n",
        "\n",
        "#### __Stakeholder:__\n",
        "\n",
        "#### __Business Case:__\n",
        "\n",
        "#### __Success Criteria:__\n",
        "We will prioritize recall in this project over accuracy. We will also aim for balance between recall (sensitivity) while maintaining a high level of precision (specificity). With these objectives in mind, we aim to reduce the number of false positives and increase the model's ability to correctly identify patients with pneumonia. In this context, false positives could lead to unnecessary treatment or interventions.\n",
        "\n",
        "\n",
        "* __True Positives (TP)__: The model correctly predicted one of the positive classes (glioma_tumor, pituitary_tumor, or meningioma_tumor).\n",
        "\n",
        "* __True Negatives (TN):__ This metric is not applicable in multi-class classification, as it is specific to binary classification where there are only two classes.\n",
        "\n",
        "* __False Positives (FP):__ The model predicted one of the positive classes, but it was incorrect.\n",
        "\n",
        "* __False Negatives (FN):__ The model failed to predict one of the positive classes.\n",
        "\n",
        "<br>\n",
        "\\begin{gathered}   \n",
        "Precision =  \\frac{True\\ Positive}{True\\ Positive + False\\ Positive}\n",
        "\\end{gathered}\n",
        "<br>\n",
        "\n",
        "</br>\n",
        "\n",
        "</br>\n",
        "\\begin{gathered}\n",
        "Recall = \\frac{True\\ Positive}{False\\ Negative + True\\ Positive}\n",
        "\\end{gathered}\n",
        "<br>\n",
        "\n",
        "A high precision indicates that when our model predicts the presence of a tumor, the patient will likely have a tumor.\n",
        "<br>"
      ],
      "metadata": {
        "id": "GpKnn92FWdJR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Understanding\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "### Data Exploration\n",
        "#### __Obtaining  Dataset for Deep Learning Analysis__\n",
        "1. __Create or Log in to Your Kaggle Account:__\n",
        "    If you do not already have a Kaggle account, create one. If you have an account log in.\n",
        "2. __Access the Pneumonia Dataset:__\n",
        "    Go to the following direct link to access dataset on Kaggle: [Dataset](https://www.kaggle.com/datasets/)\n",
        "3. __Download the Dataset:__\n",
        "    On the dataset page, you will see a \"Download\" button. Click on it to download the dataset.\n",
        "   The dataset is approximately __16MB__.\n",
        "\n",
        "4. __Unzip the file Add the unzipped archive to your Google Drive:__\n",
        "    After downloading and unzipping the dataset you'll have a folder named 'archive'. This folder contains the dataset. To use this notebook you will need to provide the location of the .zip file in your Google Drive.\n",
        "\n",
        "5. __Run the next two cells without making any chages__\n",
        "    Mount your google drive to allow colab access to your google drive.\n",
        "    Unzip the file so you can use the contents of this notebook.\n",
        "    The file should include train, test and val folders that contain subfolders with  and images.\n",
        "#### __Display basic statistics__\n",
        "#### __Check for missing values__\n",
        "\n",
        "### Data Description\n",
        "\n",
        "### Data Visualization\n"
      ],
      "metadata": {
        "id": "xmsvGlQ-lQrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell without making any changes\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "class ObtainData:\n",
        "    \"\"\"\n",
        "    A class to automatically download a dataset from Kaggle to Google Drive.\n",
        "\n",
        "    Usage:\n",
        "    data_obtainer = ObtainData()\n",
        "    data_obtainer.download_dataset(dataset_name='sartajbhuvaji/brain-tumor-classification-mri')\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.drive_mounted = False\n",
        "\n",
        "    def mount_drive(self):\n",
        "        \"\"\"\n",
        "        Mounts Google Drive to '/content/drive'.\n",
        "        \"\"\"\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        self.drive_mounted = True\n",
        "\n",
        "    def download_dataset(self, dataset_name):\n",
        "        \"\"\"\n",
        "        Downloads a dataset from Kaggle and saves it to Google Drive.\n",
        "\n",
        "        Args:\n",
        "            dataset_name (str): The name of the dataset on Kaggle (e.g., username/dataset-name).\n",
        "        \"\"\"\n",
        "\n",
        "        # Mount Google Drive if not already mounted\n",
        "        if not self.drive_mounted:\n",
        "            self.mount_drive()\n",
        "\n",
        "        # Check if the kaggle.json file exists in the 'classification' folder\n",
        "        kaggle_credentials_path = '/content/drive/MyDrive/classification/kaggle/kaggle.json'\n",
        "        if not os.path.exists(kaggle_credentials_path):\n",
        "            print(\"Please upload your Kaggle API credentials (kaggle.json) to /content/drive/MyDrive/classification/ first.\")\n",
        "            return\n",
        "\n",
        "        # Define paths\n",
        "        download_path = f'/content/drive/MyDrive/{dataset_name.split(\"/\")[1]}'\n",
        "        drive_path = f'/content/drive/MyDrive/classification/{dataset_name.split(\"/\")[1]}'\n",
        "\n",
        "        # Download dataset from Kaggle\n",
        "        !kaggle datasets download -d {dataset_name} -p {download_path}\n",
        "\n",
        "        # Unzip the downloaded file\n",
        "        !unzip -q {download_path} -d {download_path.replace(\".zip\", \"\")}\n",
        "\n",
        "        # Move dataset to Google Drive\n",
        "        os.makedirs(drive_path, exist_ok=True)\n",
        "        !mv {download_path} {drive_path}\n",
        "        print(f\"The dataset '{dataset_name}' has been downloaded and saved to Google Drive successfully.\")\n"
      ],
      "metadata": {
        "id": "v8bbENMJMdLZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell without making any changes\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "class ObtainData:\n",
        "    \"\"\"\n",
        "    A class to automatically download a dataset from Kaggle to Google Drive.\n",
        "\n",
        "    Usage:\n",
        "    data_obtainer = ObtainData()\n",
        "    data_obtainer.download_dataset(dataset_name='sartajbhuvaji/brain-tumor-classification-mri')\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.drive_mounted = False\n",
        "\n",
        "    def mount_drive(self):\n",
        "        \"\"\"\n",
        "        Mounts Google Drive to '/content/drive'.\n",
        "        \"\"\"\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        self.drive_mounted = True\n",
        "\n",
        "    def download_dataset(self, dataset_name):\n",
        "        \"\"\"\n",
        "        Downloads a dataset from Kaggle and saves it to Google Drive.\n",
        "\n",
        "        Args:\n",
        "            dataset_name (str): The name of the dataset on Kaggle (e.g., username/dataset-name).\n",
        "        \"\"\"\n",
        "\n",
        "        # Mount Google Drive if not already mounted\n",
        "        if not self.drive_mounted:\n",
        "            self.mount_drive()\n",
        "\n",
        "        # Check if the kaggle.json file exists in the 'classification' folder\n",
        "        kaggle_credentials_path = '/content/drive/MyDrive/kaggle/kaggle.json'\n",
        "        if not os.path.exists(kaggle_credentials_path):\n",
        "            print(\"Please upload your Kaggle API credentials (kaggle.json) to /root/.kaggle/ first.\")\n",
        "            return\n",
        "\n",
        "        # Define paths\n",
        "        download_path = f'/content/drive/MyDrive/{dataset_name.split(\"/\")[1]}.zip'\n",
        "        drive_path = f'/content/drive/MyDrive/classification/{dataset_name.split(\"/\")[1]}'\n",
        "\n",
        "        # Download dataset from Kaggle\n",
        "        !kaggle datasets download -d {dataset_name} -p {download_path}\n",
        "\n",
        "        # Unzip the downloaded file\n",
        "        !unzip -q {download_path} -d {download_path.replace(\".zip\", \"\")}\n",
        "\n",
        "        # Move dataset to Google Drive\n",
        "        os.makedirs(drive_path, exist_ok=True)\n",
        "        !mv {download_path.replace(\".zip\", \"\")} {drive_path}\n",
        "        print(f\"The dataset '{dataset_name}' has been downloaded, unzipped, and saved to Google Drive successfully.\")"
      ],
      "metadata": {
        "id": "YqOHlPr-buYu"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_obtainer = ObtainData()\n",
        "data_obtainer.download_dataset(dataset_name='sartajbhuvaji/brain-tumor-classification-mri')"
      ],
      "metadata": {
        "id": "BqR8ScsnPjOG",
        "outputId": "d58f3bee-de36-4c4a-ce05-12cefb70a0e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/__init__.py\", line 23, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/api/kaggle_api_extended.py\", line 403, in authenticate\n",
            "    raise IOError('Could not find {}. Make sure it\\'s located in'\n",
            "OSError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n",
            "unzip:  cannot find or open /content/drive/MyDrive/brain-tumor-classification-mri.zip, /content/drive/MyDrive/brain-tumor-classification-mri.zip.zip or /content/drive/MyDrive/brain-tumor-classification-mri.zip.ZIP.\n",
            "mv: cannot stat '/content/drive/MyDrive/brain-tumor-classification-mri': No such file or directory\n",
            "The dataset 'sartajbhuvaji/brain-tumor-classification-mri' has been downloaded, unzipped, and saved to Google Drive successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "class DatasetPaths:\n",
        "  \"\"\"\n",
        "  Helper class to manage paths for different sets and classes of the dataset\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, base_location):\n",
        "    self.base_location = base_location\n",
        "    self.class_names = ['NORMAL', 'PNEUMONIA']\n",
        "    self.set_names = ['train', 'test', 'val']\n",
        "\n",
        "  def get_single_path(self, set_name):\n",
        "      return os.path.join(self.base_location, set_name)\n",
        "  def get_path(self,set_name, class_name):\n",
        "    \"\"\"\n",
        "    Get the path for a specific set and class.\n",
        "\n",
        "    Parameters:\n",
        "        set_name(str): Name of the dataset set('train', 'test', 'val')\n",
        "        class_name(str): Name of the class ('NORMAL' or 'PNEUMONIA')\n",
        "\n",
        "    Returns:\n",
        "        path(str): Path to the specified set and class.\n",
        "    \"\"\"\n",
        "    return f\"{self.base_location}/{set_name}/{class_name}\"\n",
        "\n",
        "  def get_all_paths(self):\n",
        "    \"\"\"\n",
        "    Get a dictionary containing all paths for the dataset.\n",
        "\n",
        "    Returns:\n",
        "      paths(dict): Dictionary containing paths for each set and class.\n",
        "    \"\"\"\n",
        "\n",
        "    paths = {}\n",
        "    for set_name in self.set_names:\n",
        "        paths[set_name] = {}\n",
        "        for class_name in self.class_names:\n",
        "          paths[set_name][class_name] = self.get_path(set_name, class_name)\n",
        "    return paths"
      ],
      "metadata": {
        "id": "EVaL9lSY5kKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we used the <code>DatasetPaths</code> class to calculate and display the distributeion of images across different sets and classess within the dataset. The output displays the number of images for each combination of training, testing, and validation sets along with the two class categories: 'NORMAL' and 'PNEUMONIA'. Next, we will create a visualization to get a quick view of the dataset's composition.\n",
        "\n",
        "__ClassDistributionPlot Class Description__\n",
        "\n",
        "The <code>ClassDistributionPlot</code> class is a helper class designed to create a bar plot to visualize the distribution of classes across different dataset sets.\n",
        "\n",
        "__Features:__\n",
        "* Uses the <code>DatasetPaths</code> class to manage and access dataset paths.\n",
        "* Accepts a dictionary containing the count of images per class for each dataset set.\n",
        "* Aligns bars for each class for comparison\n",
        "* Adopts the <code>fivethirthyeight</code> style for consistency.\n",
        "\n",
        "__Usage:__\n",
        "1. Create an instance of the <code>DatasetPaths</code> class.\n",
        "2. Create an instance of the <code>ClassDistributionPlot</code> class, providing a dictionary with the class distribution data.\n",
        "3. Use the <code>plot()</code> method to generate bar plots that illustrate class distribution across different sets."
      ],
      "metadata": {
        "id": "w3l8RMowvHj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassDistributionPlot:\n",
        "    \"\"\"\n",
        "    Helper class to create class distribution plots for different dataset sets.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset_paths, set_names, class_names):\n",
        "        \"\"\"\n",
        "        Initialize the ClassDistributionPlot instance.\n",
        "\n",
        "        Parameters:\n",
        "            dataset_paths (DatasetPaths): An instance of the DatasetPaths class.\n",
        "            set_names (list): List of dataset set names ('train', 'test', 'val').\n",
        "            class_names (list): List of class names.\n",
        "        \"\"\"\n",
        "        self.dataset_paths = dataset_paths\n",
        "        self.set_names = set_names\n",
        "        self.class_names = class_names\n",
        "\n",
        "    def plot_distribution(self):\n",
        "        \"\"\"\n",
        "        Create and display class distribution plots for different dataset sets.\n",
        "        \"\"\"\n",
        "        plt.style.use('fivethirtyeight')\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        x = range(len(self.class_names))\n",
        "\n",
        "        for i, set_name in enumerate(self.set_names):\n",
        "            num_images_per_class = [\n",
        "                len(os.listdir(self.dataset_paths.get_path(set_name, class_name)))\n",
        "                for class_name in self.class_names\n",
        "            ]\n",
        "            plt.bar([j + i * 0.2 for j in x], num_images_per_class, width=0.2,\n",
        "                    align='center', label=set_name.capitalize())\n",
        "\n",
        "        plt.xlabel('Class')\n",
        "        plt.ylabel('Number of Images')\n",
        "        plt.title('Class Distribution Across Sets')\n",
        "        plt.xticks([i + 0.2 for i in x], self.class_names)\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "BeImVQTrvPfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract class names and corresponding number of images for each class\n",
        "set_names = dataset_paths.set_names\n",
        "class_names = dataset_paths.class_names\n",
        "num_images_per_class = {\n",
        "    'train': [num_images_dict['train'][class_name] for class_name in class_names],\n",
        "    'test': [num_images_dict['test'][class_name] for class_name in class_names],\n",
        "    'val': [num_images_dict['val'][class_name] for class_name in class_names]\n",
        "}\n",
        "\n",
        "# Create an instance of the ClassDistributionPlot class\n",
        "distribution_plot = ClassDistributionPlot(dataset_paths, set_names, class_names)\n",
        "\n",
        "# Plot and display class distribution\n",
        "distribution_plot.plot_distribution()"
      ],
      "metadata": {
        "id": "nFMWVL0SwFuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will implement The `ScrubData` class for preprocessing our images.\n",
        "\n",
        "### Key Features\n",
        "\n",
        "The `ScrubData` class provides the following key features:\n",
        "\n",
        "1. **Count Images**: It counts the number of images in each class for different sets (e.g., training, testing, validation) within your dataset.\n",
        "\n",
        "2. **Create Data Generator**: It creates data generators using the Keras `ImageDataGenerator` class, simplifying the configuration of data augmentation and normalization.\n",
        "\n",
        "3. **Plot Sample Images**: It enables you to visualize sample images from your dataset for a specified class, aiding in data exploration and understanding.\n",
        "\n",
        "### Usage\n",
        "\n",
        "Here's how you can use the `ScrubData` class:\n",
        "\n",
        "* Initialize the ScrubData instance with your dataset location and optional parameters\n",
        "    - <code>base_dataset_location = '/path/to/your/dataset'\n",
        "    - scrubber = ScrubData(base_dataset_location)</code>\n",
        "\n",
        "* Count images in different sets\n",
        "\n",
        "* Create data generators\n",
        "\n",
        "* Plot sample images for data exploration"
      ],
      "metadata": {
        "id": "CWC8f8N9weqR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kYA6u6O5wg8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we will perform visualizations on data that we preprocessed using the ScrubData class. To create an array of NORMAL and PNEUMONIA images and to review the pixel intensities of sample images from each class."
      ],
      "metadata": {
        "id": "-xPFcCcbwncN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "class ScrubData:\n",
        "    \"\"\"\n",
        "    The ScrubData class is responsible for managing and processing the dataset for training and testing.\n",
        "\n",
        "    Parameters:\n",
        "        base_dataset_location (str): The base directory path of the dataset.\n",
        "        image_size (tuple): The dimensions to which the images will be resized.\n",
        "        class_names (list): List of class names in the dataset.\n",
        "        batch_size_train (dict): Number of images in each class for the training set.\n",
        "        batch_size_test (dict): Number of images in each class for the test set.\n",
        "        batch_size_val (dict): Number of images in each class for the validation set.\n",
        "\n",
        "    Methods:\n",
        "        count_images(set_name): Counts the number of images in each class within a specified set.\n",
        "        create_data_generator(directory, batch_size): Creates a data generator for a specified directory.\n",
        "        plot_sample_images(data_generator, class_name, num_images, num_rows, num_cols, figsize):\n",
        "            Plots a set of sample images from the data generator.\n",
        "    \"\"\"\n",
        "    # Constructor method to initialize class attributes\n",
        "    def __init__(self, base_dataset_location, image_size=(224, 224), class_names=['NORMAL', 'PNEUMONIA']):\n",
        "        self.base_dataset_location = base_dataset_location\n",
        "        self.image_size = image_size\n",
        "        self.class_names = class_names\n",
        "\n",
        "        # Calculate batch sizes for different sets\n",
        "        self.batch_size_train = self.count_images('train_resampled')\n",
        "        self.batch_size_test = self.count_images('test')\n",
        "        self.batch_size_val = self.count_images('val')\n",
        "\n",
        "    # Method to count the number of images in each class within a set\n",
        "    def count_images(self, set_name):\n",
        "        counts = {}\n",
        "        set_path = os.path.join(self.base_dataset_location, set_name)\n",
        "\n",
        "        for class_name in self.class_names:\n",
        "            class_path = os.path.join(set_path, class_name)\n",
        "            if os.path.exists(class_path):\n",
        "                num_images = len(os.listdir(class_path))\n",
        "                counts[class_name] = num_images\n",
        "\n",
        "        return counts\n",
        "    # Method to create a data generator for a specified directory\n",
        "    def create_data_generator(self, directory, batch_size):\n",
        "        normalization_params = {\n",
        "            'NORMAL': {'rescale': 1.0 / 255.0},\n",
        "            'PNEUMONIA': {'rescale': 1.0 / 255.0}\n",
        "        }\n",
        "        # Specify class mode for the data generator\n",
        "        class_mode = 'binary'\n",
        "\n",
        "        return ImageDataGenerator(**normalization_params[class_name]).flow_from_directory(\n",
        "            directory,\n",
        "            target_size=self.image_size,\n",
        "            batch_size=batch_size,\n",
        "            class_mode=class_mode,\n",
        "            seed=42\n",
        "        )\n",
        "\n",
        "    # Method to plot sample images from a data generator\n",
        "    def plot_sample_images(self, data_generator, class_name, num_images=16, num_rows=4, num_cols=4, figsize=(10, 10)):\n",
        "        class_index = self.class_names.index(class_name)\n",
        "        class_indices = [index for index, label in enumerate(data_generator.labels) if label == class_index]\n",
        "        random_class_indices = random.sample(class_indices, num_images)\n",
        "\n",
        "        plt.figure(figsize=figsize)\n",
        "        plt.suptitle(class_name, fontsize=16)\n",
        "\n",
        "        for i, index in enumerate(random_class_indices):\n",
        "            plt.subplot(num_rows, num_cols, i + 1)\n",
        "            image = data_generator._get_batches_of_transformed_samples([index])[0][0]\n",
        "            plt.imshow(image)\n",
        "            plt.axis('off')\n",
        "\n",
        "        # Add an empty subplot for the white space\n",
        "        plt.subplot(num_rows, num_cols, num_images + 1)\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "lWjo1t0Pwo28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout,BatchNormalization, GlobalAveragePooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# Create data generators for train and test sets\n",
        "train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        ")\n",
        "\n",
        "test_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
        "val_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
        "train_data = train_generator.flow_from_directory(\n",
        "    train_data_path ,\n",
        "    target_size=input_shape[:2],\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "test_data = test_generator.flow_from_directory(\n",
        "    test_data_path,\n",
        "    target_size=input_shape[:2],\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")\n",
        "val_data = val_generator.flow_from_directory(\n",
        "    val_data_path,\n",
        "    target_size=input_shape[:2],\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "tE_15LyZym93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modeling\n"
      ],
      "metadata": {
        "id": "FpA58RjL5rMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create the baseline CNN model\n",
        "cnn_model_with_callbacks = tf.keras.models.Sequential([\n",
        "\n",
        "    Conv2D(16, (3, 3), activation='relu', padding=\"same\", input_shape=input_shape),\n",
        "    Conv2D(16, (3, 3), padding=\"same\", activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "\n",
        "    Conv2D(32, (3, 3), activation='relu', padding=\"same\"),\n",
        "    Conv2D(32, (3, 3), padding=\"same\", activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu', padding=\"same\"),\n",
        "    Conv2D(64, (3, 3), padding=\"same\", activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "\n",
        "    Conv2D(96, (3, 3), dilation_rate=(2, 2), activation='relu', padding=\"same\"),\n",
        "    Conv2D(96, (3, 3), padding=\"valid\", activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "\n",
        "    Conv2D(128, (3, 3), dilation_rate=(2, 2), activation='relu', padding=\"same\"),\n",
        "    Conv2D(128, (3, 3), padding=\"valid\", activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(units=256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(units=1, activation='sigmoid'),\n",
        "\n",
        "])\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "cnn_model_with_callbacks.compile(optimizer='Adam',\n",
        "                       loss='binary_crossentropy',\n",
        "                       metrics=metrics)\n",
        "class_weight = {0:0.93, 1: 0.68}\n",
        "# Train the model\n",
        "history = cnn_model_with_callbacks.fit(\n",
        "    train_data,\n",
        "    steps_per_epoch=train_data.samples // train_data.batch_size,\n",
        "    epochs=50, #increased epochs\n",
        "    validation_data=val_data,\n",
        "    validation_steps=val_data.samples // val_data.batch_size,\n",
        "    #class_weight= class_weight,\n",
        "    callbacks=[reduce_lr, early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluate the model on the testdata\n",
        "val_loss, val_precision, val_accuracy, val_recall = cnn_model_with_callbacks.evaluate(val_data)\n",
        "print(f\"Val Loss: {val_loss:.4f}, Val Precision: {val_precision:.4f},Val Accuracy: {val_accuracy:.4f}, Val Recall: {val_recall:.4f}\")\n",
        "\n",
        "# Predict labels for the val data\n",
        "y_pred = cnn_model_with_callbacks.predict(val_data)\n",
        "y_pred_binary = np.argmax(y_pred, axis=1)  # Convert probabilities to binary predictions\n",
        "\n",
        "# True labels for the val data\n",
        "y_true = test_data.classes"
      ],
      "metadata": {
        "id": "e7XHH1qP0o4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the summary of the model\n",
        "\n",
        "cnn_model_with_callbacks.summary()\n",
        "\n",
        "# Print the summary of the model\n",
        "\n",
        "cnn_model_with_callbacks.summary()"
      ],
      "metadata": {
        "id": "o9-d9riY0Z2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n"
      ],
      "metadata": {
        "id": "dn_leZJn5zk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = val_data.classes\n",
        "y_pred = cnn_model_with_callbacks.predict(val_data)\n",
        "threshold = 0.5\n",
        "y_pred_binary = (y_pred > threshold).astype(int)\n",
        "y_true = np.concatenate([y_true for _ in range(y_pred_binary.shape[0] // len(y_true))])  # Match shapes\n",
        "\n",
        "# Now compute the confusion matrix\n",
        "confusion = confusion_matrix(y_true, y_pred_binary)\n",
        "\n",
        "# Plot the confusion matrix as a heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.set(font_scale=1.2)\n",
        "sns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
        "            xticklabels=val_data.class_indices.keys(),\n",
        "            yticklabels=val_data.class_indices.keys())\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix: CNN with callbacks Validation Set')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AwXLzrr452d1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deployment"
      ],
      "metadata": {
        "id": "CW8MxReL52_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install kaggle\n",
        "!pip install kaggle"
      ],
      "metadata": {
        "id": "-csn0D_w54RN",
        "outputId": "7823274c-2a8c-4cf8-df41-144a63e816a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.0.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "id": "VhiN_49w15ml",
        "outputId": "5ac46a22-d507-4196-97ee-187a0052a6c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /root/.kaggle\n",
        "!cp /content/drive/MyDrive/kaggle/kaggle.json /root/.kaggle/kaggle.json\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n"
      ],
      "metadata": {
        "id": "1KXwGzIx2N-Z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download -d sartajbhuvaji/brain-tumor-classification-mri\n"
      ],
      "metadata": {
        "id": "Y14KgqyT4I7F",
        "outputId": "bd7c51f3-506a-4e1b-db18-b1ed0023de1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading brain-tumor-classification-mri.zip to /content\n",
            " 89% 77.0M/86.8M [00:00<00:00, 234MB/s]\n",
            "100% 86.8M/86.8M [00:00<00:00, 202MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q brain-tumor-classification-mri.zip -d /content/drive/MyDrive/brain_tumor_classification_mri\n"
      ],
      "metadata": {
        "id": "qR9BcFcJ5JUf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7_kFt2Nn6Otf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-LW3pfbF5S0m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}