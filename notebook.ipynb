{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyONgguSZdPq2sAgMz/Lke6D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dataeducator/capstone/blob/main/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Project Submission:Capstone\n",
        "(Capstone)\n",
        "\n",
        "- Student Name: Tenicka Norwood\n",
        "- Program Pace: self-paced\n",
        "- Scheduled Project Review Time: Tuesday, September 19, 2023, 12 pm\n",
        "- Instructor name: Morgan Jones\n",
        "- Blog post Url: https://medium.com/mlearning-ai/fueling-student-success-1723abd2991b"
      ],
      "metadata": {
        "id": "cdn2T52V4tmC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project I will be using __CRISP-DM__ process which has six phases:\n",
        "\n",
        "* Business Understanding __&#8594;__ Understanding the project objectives, requirements, and constraints from a business perspective.\n",
        "\n",
        "* Data Understanding __&#8594;__ Exploring and assessing the available data, its quality, structure, and initial insights.\n",
        "\n",
        "* Data Preparation __&#8594;__ Cleaning, transforming, and preparing the data to be used for modeling, including handling missing values and outliers.\n",
        "\n",
        "* Modeling __&#8594;__ Selecting and applying appropriate machine learning algorithms or techniques to build predictive or descriptive models.\n",
        "\n",
        "* Evaluation __&#8594;__ Assessing the performance of the models and determining their suitability for solving the business problem.\n",
        "\n",
        "* Deployment __&#8594;__ Integrating the chosen model into the business environment, making it accessible for end-users."
      ],
      "metadata": {
        "id": "tg7jRLIIQRaJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Business Understanding\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0LQdzJxA5BS5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### __Disclaimer:__\n",
        "This Jupyter notebook and its contents are __intended solely for educational purposes__. The included business case and the results of the deep learning models should not be interpreted as medical advice, and have not received endorsement or approval from any professional or medical organization.\n",
        "\n",
        "The models and outcomes presented here are for illustrative purposes __only__. Users should __not__ use these models or their outcomes for making real-world decisions without consulting appropriate domain experts and medical professionals. Any actions taken based on the information in this notebook are at the user's own risk. The dataset may not be fully representative of real-world clinical scenarios and should be used with caution in clinical decision-making. Always consult a qualified healthcare professional for medical advice and treatment.\n",
        "The author and contributors of this notebook disclaim any liability for the accuracy, completeness, or efficacy of the information provided.\n",
        "\n",
        "#### __Overview:__\n",
        "\n",
        "The primary objective of this project is to develop a robust predictive model that is capable of accurately identifying individuals at high risk of heart failure based on demographic and clinal data. By harnessing the power of machine learning, Zephyr Healthcare Solutionss aims to signifantly improve patient hospitalizations, and ehannce overal quality of their cardiac care.\n",
        "\n",
        "#### __Objectives and Goals:__\n",
        "Zephyr's objective is to leverage machine learning models including optimized neural network based models for precise heart disease detection, aligning within our commitment to advancing diagnostic accuracy and patient care.\n",
        "\n",
        "#### __Problem Statement:__\n",
        "In the realm diagnostics, timely and accurate diseease detection is crucial to improving patient health outcomes. Zephyr Healthcare recognizes the potential of machine learning techniques in achieving this goal. The challenge lies in developing a robust predictive model that harnesses the power of neural networks or ensemble methods to identify cardiac conditions with a focus on achieving high recall rates.\n",
        "\n",
        "#### __Stakeholder:__\n",
        "Zephyr Healthcare Solutions\n",
        "\n",
        "#### __Business Case:__  \n",
        "As a newly appointed lead of the data analytics team at Zephyr Healthcare Solutions,my team has been tasked with enhancing the company's diagnostic capabilities through advanced predictive modeling techniques.\n",
        "\n",
        "#### __Success Criteria:__\n",
        "We will prioritize recall in this project over accuracy. We will also aim for balance between recall (sensitivity) while maintaining a high level of precision (specificity). With these objectives in mind, we aim to reduce the number of false positives and increase the model's ability to correctly identify patients with pneumonia. In this context, false positives could lead to unnecessary treatment or interventions.\n",
        "\n",
        "\n",
        "* __True Positives (TP)__: This represents the number of instances that the model correctly predicted that a person is at risk for heart disease.\n",
        "\n",
        "* __True Negatives (TN):__ This represents the number of instances that the model correctly predicted that a person is not at risk for heart disease.\n",
        "\n",
        "* __False Positives (FP):__ This represents the number of instances where the model incorrectly predicted that a person is at risk for heart disease, when they are not at risk.\n",
        "\n",
        "* __False Negatives (FN):__ This represents the numbeer of instances where the model incorrectly is not at risk for heart disease, when they are at risk.\n",
        "\n",
        "<br>\n",
        "\\begin{gathered}   \n",
        "Precision =  \\frac{True\\ Positive}{True\\ Positive + False\\ Positive}\n",
        "\\end{gathered}\n",
        "<br>\n",
        "\n",
        "</br>\n",
        "\n",
        "</br>\n",
        "\\begin{gathered}\n",
        "Recall = \\frac{True\\ Positive}{False\\ Negative + True\\ Positive}\n",
        "\\end{gathered}\n",
        "<br>\n",
        "\n",
        "A high precision indicates that when our model predicts the presence of a tumor, the patient will likely have a tumor.\n",
        "<br>"
      ],
      "metadata": {
        "id": "GpKnn92FWdJR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Understanding\n",
        "\n",
        "---\n",
        "### Data Exploration\n",
        "#### __Obtaining  Dataset for Prediction with Machine  Learning__\n",
        "1. __Create or Log in to Your Kaggle Account:__\n",
        "    If you do not already have a Kaggle account, create one. If you have an account log in.\n",
        "2. __Access Your Account Settings:__\n",
        "  - Click on your profile picture in the top right corner of the Kaggle website.\n",
        "  - Select __`Account`__ from the dropdown menu.\n",
        "    \n",
        "3. __Navigate to the API Section:__\n",
        "  - Scroll down to the __`API`__ section on the account page.\n",
        "\n",
        "4. __Create New API Token:__\n",
        "  - Click on the __`Create New API Token`__ button. This will trigger the download of a file named `kaggle.json`.\n",
        "5. __Move API Token to Google Drive(We will be using Google Colab)__\n",
        " - We will be using Gogle Colab. Please upload the `kaggle.json` file to a folder called kaggle your Gogle Drive. This will allow you to access the Kaggle API from your Colab notebooks.\n",
        "\n"
      ],
      "metadata": {
        "id": "xmsvGlQ-lQrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount your Google Drive to '/content/drive'\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create a directory name '.kaggle' in the root directory\n",
        "!mkdir /root/.kaggle\n",
        "\n",
        "# Copy the 'kaggle.json' file from the specified location in Google Drive to the kaggle directory\n",
        "# !cp /content/drive/MyDrive/your_folder_name/kaggle.json\n",
        "!cp /content/drive/MyDrive/kaggle/kaggle.json /root/.kaggle/kaggle.json\n",
        "\n",
        "# Set permissions to protect your API key and only the owner can use the credentials held within your json file\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "# Download the dataset 'heart-failure-predition' using the Kaggle API\n",
        "!kaggle datasets download -d fedesoriano/heart-failure-prediction\n",
        "\n",
        "# Unzip the downloaded dataset and place it in the specified location in Google Drive\n",
        "!unzip heart-failure-prediction.zip -d /content/drive/MyDrive/kaggle/heart-failure-prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "eOuzfXYyfRjm",
        "outputId": "2488dc54-413e-4855-ab14-1deceebca350",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "Downloading heart-failure-prediction.zip to /content\n",
            "  0% 0.00/8.56k [00:00<?, ?B/s]\n",
            "100% 8.56k/8.56k [00:00<00:00, 21.1MB/s]\n",
            "Archive:  heart-failure-prediction.zip\n",
            "  inflating: /content/drive/MyDrive/kaggle/heart-failure-prediction/heart.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries and visualization packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import statsmodels.api as sm\n",
        "import sklearn\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import make_scorer, recall_score\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.metrics import BinaryAccuracy, Recall\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.simplefilter(action ='ignore', category = DeprecationWarning)\n",
        "warnings.simplefilter(action ='ignore', category = FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pandas\")\n",
        "\n",
        "\n",
        "# Allow plots to display and be stored inline within a notebook\n",
        "%matplotlib inline\n",
        "\n",
        "# Used for working with the z-score\n",
        "from scipy import stats\n",
        "\n",
        "# Used for working with long url\n",
        "from urllib.parse import urlencode\n",
        "\n",
        "# Set display option to readable format\n",
        "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
        "\n",
        "# Filter warnings from pandas\n",
        "pd.options.mode.chained_assignment = None"
      ],
      "metadata": {
        "id": "Hate4-Xki2fd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Pandas version\n",
        "print(\"Pandas version\")\n",
        "pd.__version__"
      ],
      "metadata": {
        "id": "xsJ5ciG4jiz1",
        "outputId": "07b79d30-84f4-41f1-ec49-6154b10e1d31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pandas version\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.5.3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Numpy version\n",
        "print(\"Numpy version\")\n",
        "np.__version__"
      ],
      "metadata": {
        "id": "t05aECpfjns7",
        "outputId": "1166c258-5c19-4b44-def7-59ecd5337b36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numpy version\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.23.5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Seaborn version\n",
        "print(\"Seaborn version\")\n",
        "sns.__version__"
      ],
      "metadata": {
        "id": "qeWHDzK6jrQ8",
        "outputId": "adfc0486-0a83-4c5e-8d9d-c3afc97c7e00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seaborn version\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.12.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Tensorflow version\n",
        "print(\"Tensorflow version\")\n",
        "tf.__version__"
      ],
      "metadata": {
        "id": "dTkwhoPNjvVW",
        "outputId": "076df755-9cb0-48b1-bbe4-b22579780168",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow version\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.13.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### __Display basic statistics__\n",
        "#### __Check for missing values__\n",
        "\n",
        "### __Data Description:__\n",
        "The Heart Failure Prediction dataset is a collection of clinical and demographic features that was created by combining five heart datasets aimed at predicting the likelihood of heart failure.\n",
        "\n",
        "#### Features\n",
        "| Feature                 | Data Type |Description                          |\n",
        "|----------------------------|-----------|----------------------------------|\n",
        "| `age`                      | Numeric   | Age of the patient in years.                                        |\n",
        "| `anaemia`                  | Binary    | Indicates whether the patient has anaemia (0 for no, 1 for yes).    |\n",
        "| `creatinine_phosphokinase` | Numeric   | Level of creatinine phosphokinase enzyme in the blood (mcg/L).      |\n",
        "| `diabetes`                 | Binary    | Indicates whether the patient has diabetes (0 for no, 1 for yes).   |\n",
        "| `ejection_fraction`        | Numeric   | Percentage of blood leaving the heart at each contraction.          |\n",
        "| `high_blood_pressure`      | Binary    | Indicates whether the patient has high blood pressure (0 for no, 1 for yes). |\n",
        "| `platelets`                | Numeric   | Platelets count in the blood (kiloplatelets/mL).                    |\n",
        "| `serum_creatinine`         | Numeric   | Level of serum creatinine in the blood (mg/dL).                     |\n",
        "| `serum_sodium`             | Numeric   | Level of serum sodium in the blood (mEq/L).                         |\n",
        "| `sex`                      | Binary    | Gender of the patient (0 for female, 1 for male).                   |\n",
        "| `smoking`                  | Binary    | Indicates whether the patient is a smoker (0 for no, 1 for yes).    |\n",
        "| `time`                     | Numeric   | Follow-up period in days.                                          |\n",
        "| `DEATH_EVENT`              | Binary    | Indicates whether the patient experienced a death\n",
        "\n",
        "### __Data Source__\n",
        "The dataset was compiled by Fedesoriano on Kaggle from a set of datasets included datasets that were orginially hosted on the University of California at Irvine's Machine Learning Inventory [click here](https://archive.ics.uci.edu/dataset/45/heart+disease)\n",
        "####__Sources:__\n",
        "Janosi,Andras, Steinbrunn,William, Pfisterer,Matthias, and Detrano,Robert. (1988). Heart Disease. UCI Machine Learning Repository. https://doi.org/10.24432/C52P4X.\n",
        "\n",
        "fedesoriano. (September 2021). Heart Failure Prediction Dataset. Retrieved [September 17, 2023] from https://www.kaggle.com/fedesoriano/heart-failure-prediction.\n",
        "### Data Visualization"
      ],
      "metadata": {
        "id": "Z9gJgh8HfQbV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we used the <code>DatasetPaths</code> class to calculate and display the distributeion of images across different sets and classess within the dataset. The output displays the number of images for each combination of training, testing, and validation sets along with the two class categories: 'NORMAL' and 'PNEUMONIA'. Next, we will create a visualization to get a quick view of the dataset's composition.\n",
        "\n",
        "__ClassDistributionPlot Class Description__\n",
        "\n",
        "The <code>ClassDistributionPlot</code> class is a helper class designed to create a bar plot to visualize the distribution of classes across different dataset sets.\n",
        "\n",
        "__Features:__\n",
        "* Uses the <code>DatasetPaths</code> class to manage and access dataset paths.\n",
        "* Accepts a dictionary containing the count of images per class for each dataset set.\n",
        "* Aligns bars for each class for comparison\n",
        "* Adopts the <code>fivethirthyeight</code> style for consistency.\n",
        "\n",
        "__Usage:__\n",
        "1. Create an instance of the <code>DatasetPaths</code> class.\n",
        "2. Create an instance of the <code>ClassDistributionPlot</code> class, providing a dictionary with the class distribution data.\n",
        "3. Use the <code>plot()</code> method to generate bar plots that illustrate class distribution across different sets."
      ],
      "metadata": {
        "id": "w3l8RMowvHj_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n"
      ],
      "metadata": {
        "id": "dn_leZJn5zk2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deployment"
      ],
      "metadata": {
        "id": "CW8MxReL52_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ],
      "metadata": {
        "id": "JI42tIUiNZHp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Future Work\n"
      ],
      "metadata": {
        "id": "eU0Wnt8LNamL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### References\n",
        "\n",
        "International application of a new probability algorithm for the diagnosis of coronary artery disease.\n",
        "By R. Detrano, A. Jánosi, W. Steinbrunn, M. Pfisterer, J. Schmid, S. Sandhu, K. Guppy, S. Lee, V. Froelicher. 1989\n",
        "\n",
        "Published in American Journal of Cardiology"
      ],
      "metadata": {
        "id": "_QzVhTS7XVzi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zuw1Iz-_Nclm"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}