{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMB0+oUVIjeMWeGxSsqalj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dataeducator/capstone/blob/main/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Project Submission:Capstone\n",
        "(Capstone)\n",
        "\n",
        "- Student Name: Tenicka Norwood\n",
        "- Program Pace: self-paced\n",
        "- Scheduled Project Review Time: Tuesday, September 19, 2023, 12 pm\n",
        "- Instructor name: Morgan Jones\n",
        "- Blog post Url: https://medium.com/mlearning-ai/fueling-student-success-1723abd2991b"
      ],
      "metadata": {
        "id": "cdn2T52V4tmC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project I will be using __CRISP-DM__ process which has six phases:\n",
        "\n",
        "* Business Understanding __&#8594;__ Understanding the project objectives, requirements, and constraints from a business perspective.\n",
        "\n",
        "* Data Understanding __&#8594;__ Exploring and assessing the available data, its quality, structure, and initial insights.\n",
        "\n",
        "* Data Preparation __&#8594;__ Cleaning, transforming, and preparing the data to be used for modeling, including handling missing values and outliers.\n",
        "\n",
        "* Modeling __&#8594;__ Selecting and applying appropriate machine learning algorithms or techniques to build predictive or descriptive models.\n",
        "\n",
        "* Evaluation __&#8594;__ Assessing the performance of the models and determining their suitability for solving the business problem.\n",
        "\n",
        "* Deployment __&#8594;__ Integrating the chosen model into the business environment, making it accessible for end-users."
      ],
      "metadata": {
        "id": "tg7jRLIIQRaJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Business Understanding\n"
      ],
      "metadata": {
        "id": "0LQdzJxA5BS5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### __Disclaimer:__\n",
        "This Jupyter notebook and its contents are __intended solely for educational purposes__. The included business case and the results of the deep learning models should not be interpreted as medical advice, and have not received endorsement or approval from any professional or medical organization.\n",
        "\n",
        "The models and outcomes presented here are for illustrative purposes __only__. Users should __not__ use these models or their outcomes for making real-world decisions without consulting appropriate domain experts and medical professionals. Any actions taken based on the information in this notebook are at the user's own risk.\n",
        "The author and contributors of this notebook disclaim any liability for the accuracy, completeness, or efficacy of the information provided."
      ],
      "metadata": {
        "id": "GpKnn92FWdJR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Understanding\n",
        "\n",
        "## __Metrics__\n",
        "We will prioritize recall in this project over accuracy. We will also aim for balance between recall (sensitivity) while maintaining a high level of precision (specificity). With these objectives in mind, we aim to reduce the number of false positives and increase the model's ability to correctly identify patients with pneumonia. In this context, false positives could lead to unnecessary treatment or interventions.\n",
        "\n",
        "\n",
        "* __True Positives (TP)__: The model correctly predicted one of the positive classes (glioma_tumor, pituitary_tumor, or meningioma_tumor).\n",
        "\n",
        "* __True Negatives (TN):__ This metric is not applicable in multi-class classification, as it is specific to binary classification where there are only two classes.\n",
        "\n",
        "* __False Positives (FP):__ The model predicted one of the positive classes, but it was incorrect.\n",
        "\n",
        "* __False Negatives (FN):__ The model failed to predict one of the positive classes.\n",
        "\n",
        "<br>\n",
        "\\begin{gathered}   \n",
        "Precision =  \\frac{True\\ Positive}{True\\ Positive + False\\ Positive}\n",
        "\\end{gathered}\n",
        "<br>\n",
        "\n",
        "</br>\n",
        "\n",
        "</br>\n",
        "\\begin{gathered}\n",
        "Recall = \\frac{True\\ Positive}{False\\ Negative + True\\ Positive}\n",
        "\\end{gathered}\n",
        "<br>\n",
        "\n",
        "A high precision indicates that when our model predicts the presence of a tumor, the patient will likely have a tumor.\n",
        "<br>"
      ],
      "metadata": {
        "id": "3axmg4wq5W_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading and Preparing  Dataset for Deep Learning Analysis\n",
        "1. __Create or Log in to Your Kaggle Account:__\n",
        "    If you do not already have a Kaggle account, create one. If you have an account log in.\n",
        "2. __Access the Pneumonia Dataset:__\n",
        "    Go to the following direct link to access dataset on Kaggle: [Dataset](https://www.kaggle.com/datasets/)\n",
        "3. __Download the Dataset:__\n",
        "    On the dataset page, you will see a \"Download\" button. Click on it to download the dataset.\n",
        "   The dataset is approximately __2GB__.\n",
        "\n",
        "4. __Unzip the file Add the unzipped archive to your Google Drive:__\n",
        "    After downloading and unzipping the dataset you'll have a folder named 'archive'. This folder contains the dataset. To use this notebook you will need to provide the location of the .zip file in your Google Drive.\n",
        "\n",
        "5. __Run the next two cells without making any chages__\n",
        "    Mount your google drive to allow colab access to your google drive.\n",
        "    Unzip the file so you can use the contents of this notebook.\n",
        "    The file should include train, test and val folders that contain subfolders with  and images."
      ],
      "metadata": {
        "id": "xmsvGlQ-lQrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run cell without making any changes\n",
        "from google.colab import drive\n",
        "\n",
        "class ObtainData:\n",
        "  \"\"\"\n",
        "  A class to obtain dataset location from Google Drive.\n",
        "\n",
        "  Usage:\n",
        "  data_obtainer = ObtainData()\n",
        "  dataset_location = data_obtainer.get_dataset_location()\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "      self.drive_mounted = False\n",
        "\n",
        "  def mount_drive(self):\n",
        "    \"\"\"\n",
        "    Mounts Google Drive to '/content/drive'.\n",
        "    \"\"\"\n",
        "    drive.mount('/content/drive')\n",
        "    self.drive_mounted = True\n",
        "\n",
        "  def get_dataset_location(self):\n",
        "    \"\"\"\n",
        "    Prompts the user to enter the location of the datset folder in their Google Drive.\n",
        "    Returns the full file path of the dataset location.\n",
        "    \"\"\"\n",
        "\n",
        "    while True:\n",
        "      # Munt Google Drive if not already mounted\n",
        "      if not self.drive_mounted:\n",
        "        self.mount_drive()\n",
        "\n",
        "      # Provide a template for the user input\n",
        "      example_input =\"/MyDrive/Your_Folder_name/\"\n",
        "      dataset_location = input(f\"Enter the location of the dataset folder in your Google Drive (e.g., {example_input}):\")\n",
        "      file_path = f'/content/drive{dataset_location}'\n",
        "\n",
        "      # Check if the file exists\n",
        "      if os.path.exists(file_path):\n",
        "        print(f\" The file '{dataset_location}' exists in your Google Drive.\")\n",
        "        return file_path\n",
        "      else:\n",
        "        print(f\" The file '{dataset_location}' does not exist in your Google Drive. Please try again.\")"
      ],
      "metadata": {
        "id": "RXcB4Zby5gZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run cell without making any changes\n",
        "# Create an instance of the ObtainData class\n",
        "data_obtainer = ObtainData()\n",
        "\n",
        "# Get the dataset location\n",
        "dataset_location = data_obtainer.get_dataset_location()\n",
        "print(f\"Datatset location\")"
      ],
      "metadata": {
        "id": "xs4xRzGbrzAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation\n"
      ],
      "metadata": {
        "id": "0YWh9ixf5h9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "class DatasetPaths:\n",
        "  \"\"\"\n",
        "  Helper class to manage paths for different sets and classes of the dataset\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, base_location):\n",
        "    self.base_location = base_location\n",
        "    self.class_names = ['NORMAL', 'PNEUMONIA']\n",
        "    self.set_names = ['train', 'test', 'val']\n",
        "\n",
        "  def get_single_path(self, set_name):\n",
        "      return os.path.join(self.base_location, set_name)\n",
        "  def get_path(self,set_name, class_name):\n",
        "    \"\"\"\n",
        "    Get the path for a specific set and class.\n",
        "\n",
        "    Parameters:\n",
        "        set_name(str): Name of the dataset set('train', 'test', 'val')\n",
        "        class_name(str): Name of the class ('NORMAL' or 'PNEUMONIA')\n",
        "\n",
        "    Returns:\n",
        "        path(str): Path to the specified set and class.\n",
        "    \"\"\"\n",
        "    return f\"{self.base_location}/{set_name}/{class_name}\"\n",
        "\n",
        "  def get_all_paths(self):\n",
        "    \"\"\"\n",
        "    Get a dictionary containing all paths for the dataset.\n",
        "\n",
        "    Returns:\n",
        "      paths(dict): Dictionary containing paths for each set and class.\n",
        "    \"\"\"\n",
        "\n",
        "    paths = {}\n",
        "    for set_name in self.set_names:\n",
        "        paths[set_name] = {}\n",
        "        for class_name in self.class_names:\n",
        "          paths[set_name][class_name] = self.get_path(set_name, class_name)\n",
        "    return paths"
      ],
      "metadata": {
        "id": "EVaL9lSY5kKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we used the <code>DatasetPaths</code> class to calculate and display the distributeion of images across different sets and classess within the dataset. The output displays the number of images for each combination of training, testing, and validation sets along with the two class categories: 'NORMAL' and 'PNEUMONIA'. Next, we will create a visualization to get a quick view of the dataset's composition.\n",
        "\n",
        "__ClassDistributionPlot Class Description__\n",
        "\n",
        "The <code>ClassDistributionPlot</code> class is a helper class designed to create a bar plot to visualize the distribution of classes across different dataset sets.\n",
        "\n",
        "__Features:__\n",
        "* Uses the <code>DatasetPaths</code> class to manage and access dataset paths.\n",
        "* Accepts a dictionary containing the count of images per class for each dataset set.\n",
        "* Aligns bars for each class for comparison\n",
        "* Adopts the <code>fivethirthyeight</code> style for consistency.\n",
        "\n",
        "__Usage:__\n",
        "1. Create an instance of the <code>DatasetPaths</code> class.\n",
        "2. Create an instance of the <code>ClassDistributionPlot</code> class, providing a dictionary with the class distribution data.\n",
        "3. Use the <code>plot()</code> method to generate bar plots that illustrate class distribution across different sets."
      ],
      "metadata": {
        "id": "w3l8RMowvHj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassDistributionPlot:\n",
        "    \"\"\"\n",
        "    Helper class to create class distribution plots for different dataset sets.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset_paths, set_names, class_names):\n",
        "        \"\"\"\n",
        "        Initialize the ClassDistributionPlot instance.\n",
        "\n",
        "        Parameters:\n",
        "            dataset_paths (DatasetPaths): An instance of the DatasetPaths class.\n",
        "            set_names (list): List of dataset set names ('train', 'test', 'val').\n",
        "            class_names (list): List of class names.\n",
        "        \"\"\"\n",
        "        self.dataset_paths = dataset_paths\n",
        "        self.set_names = set_names\n",
        "        self.class_names = class_names\n",
        "\n",
        "    def plot_distribution(self):\n",
        "        \"\"\"\n",
        "        Create and display class distribution plots for different dataset sets.\n",
        "        \"\"\"\n",
        "        plt.style.use('fivethirtyeight')\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        x = range(len(self.class_names))\n",
        "\n",
        "        for i, set_name in enumerate(self.set_names):\n",
        "            num_images_per_class = [\n",
        "                len(os.listdir(self.dataset_paths.get_path(set_name, class_name)))\n",
        "                for class_name in self.class_names\n",
        "            ]\n",
        "            plt.bar([j + i * 0.2 for j in x], num_images_per_class, width=0.2,\n",
        "                    align='center', label=set_name.capitalize())\n",
        "\n",
        "        plt.xlabel('Class')\n",
        "        plt.ylabel('Number of Images')\n",
        "        plt.title('Class Distribution Across Sets')\n",
        "        plt.xticks([i + 0.2 for i in x], self.class_names)\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "BeImVQTrvPfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract class names and corresponding number of images for each class\n",
        "set_names = dataset_paths.set_names\n",
        "class_names = dataset_paths.class_names\n",
        "num_images_per_class = {\n",
        "    'train': [num_images_dict['train'][class_name] for class_name in class_names],\n",
        "    'test': [num_images_dict['test'][class_name] for class_name in class_names],\n",
        "    'val': [num_images_dict['val'][class_name] for class_name in class_names]\n",
        "}\n",
        "\n",
        "# Create an instance of the ClassDistributionPlot class\n",
        "distribution_plot = ClassDistributionPlot(dataset_paths, set_names, class_names)\n",
        "\n",
        "# Plot and display class distribution\n",
        "distribution_plot.plot_distribution()"
      ],
      "metadata": {
        "id": "nFMWVL0SwFuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will implement The `ScrubData` class for preprocessing our images.\n",
        "\n",
        "### Key Features\n",
        "\n",
        "The `ScrubData` class provides the following key features:\n",
        "\n",
        "1. **Count Images**: It counts the number of images in each class for different sets (e.g., training, testing, validation) within your dataset.\n",
        "\n",
        "2. **Create Data Generator**: It creates data generators using the Keras `ImageDataGenerator` class, simplifying the configuration of data augmentation and normalization.\n",
        "\n",
        "3. **Plot Sample Images**: It enables you to visualize sample images from your dataset for a specified class, aiding in data exploration and understanding.\n",
        "\n",
        "### Usage\n",
        "\n",
        "Here's how you can use the `ScrubData` class:\n",
        "\n",
        "* Initialize the ScrubData instance with your dataset location and optional parameters\n",
        "    - <code>base_dataset_location = '/path/to/your/dataset'\n",
        "    - scrubber = ScrubData(base_dataset_location)</code>\n",
        "\n",
        "* Count images in different sets\n",
        "\n",
        "* Create data generators\n",
        "\n",
        "* Plot sample images for data exploration"
      ],
      "metadata": {
        "id": "CWC8f8N9weqR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kYA6u6O5wg8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we will perform visualizations on data that we preprocessed using the ScrubData class. To create an array of NORMAL and PNEUMONIA images and to review the pixel intensities of sample images from each class."
      ],
      "metadata": {
        "id": "-xPFcCcbwncN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "class ScrubData:\n",
        "    \"\"\"\n",
        "    The ScrubData class is responsible for managing and processing the dataset for training and testing.\n",
        "\n",
        "    Parameters:\n",
        "        base_dataset_location (str): The base directory path of the dataset.\n",
        "        image_size (tuple): The dimensions to which the images will be resized.\n",
        "        class_names (list): List of class names in the dataset.\n",
        "        batch_size_train (dict): Number of images in each class for the training set.\n",
        "        batch_size_test (dict): Number of images in each class for the test set.\n",
        "        batch_size_val (dict): Number of images in each class for the validation set.\n",
        "\n",
        "    Methods:\n",
        "        count_images(set_name): Counts the number of images in each class within a specified set.\n",
        "        create_data_generator(directory, batch_size): Creates a data generator for a specified directory.\n",
        "        plot_sample_images(data_generator, class_name, num_images, num_rows, num_cols, figsize):\n",
        "            Plots a set of sample images from the data generator.\n",
        "    \"\"\"\n",
        "    # Constructor method to initialize class attributes\n",
        "    def __init__(self, base_dataset_location, image_size=(224, 224), class_names=['NORMAL', 'PNEUMONIA']):\n",
        "        self.base_dataset_location = base_dataset_location\n",
        "        self.image_size = image_size\n",
        "        self.class_names = class_names\n",
        "\n",
        "        # Calculate batch sizes for different sets\n",
        "        self.batch_size_train = self.count_images('train_resampled')\n",
        "        self.batch_size_test = self.count_images('test')\n",
        "        self.batch_size_val = self.count_images('val')\n",
        "\n",
        "    # Method to count the number of images in each class within a set\n",
        "    def count_images(self, set_name):\n",
        "        counts = {}\n",
        "        set_path = os.path.join(self.base_dataset_location, set_name)\n",
        "\n",
        "        for class_name in self.class_names:\n",
        "            class_path = os.path.join(set_path, class_name)\n",
        "            if os.path.exists(class_path):\n",
        "                num_images = len(os.listdir(class_path))\n",
        "                counts[class_name] = num_images\n",
        "\n",
        "        return counts\n",
        "    # Method to create a data generator for a specified directory\n",
        "    def create_data_generator(self, directory, batch_size):\n",
        "        normalization_params = {\n",
        "            'NORMAL': {'rescale': 1.0 / 255.0},\n",
        "            'PNEUMONIA': {'rescale': 1.0 / 255.0}\n",
        "        }\n",
        "        # Specify class mode for the data generator\n",
        "        class_mode = 'binary'\n",
        "\n",
        "        return ImageDataGenerator(**normalization_params[class_name]).flow_from_directory(\n",
        "            directory,\n",
        "            target_size=self.image_size,\n",
        "            batch_size=batch_size,\n",
        "            class_mode=class_mode,\n",
        "            seed=42\n",
        "        )\n",
        "\n",
        "    # Method to plot sample images from a data generator\n",
        "    def plot_sample_images(self, data_generator, class_name, num_images=16, num_rows=4, num_cols=4, figsize=(10, 10)):\n",
        "        class_index = self.class_names.index(class_name)\n",
        "        class_indices = [index for index, label in enumerate(data_generator.labels) if label == class_index]\n",
        "        random_class_indices = random.sample(class_indices, num_images)\n",
        "\n",
        "        plt.figure(figsize=figsize)\n",
        "        plt.suptitle(class_name, fontsize=16)\n",
        "\n",
        "        for i, index in enumerate(random_class_indices):\n",
        "            plt.subplot(num_rows, num_cols, i + 1)\n",
        "            image = data_generator._get_batches_of_transformed_samples([index])[0][0]\n",
        "            plt.imshow(image)\n",
        "            plt.axis('off')\n",
        "\n",
        "        # Add an empty subplot for the white space\n",
        "        plt.subplot(num_rows, num_cols, num_images + 1)\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "lWjo1t0Pwo28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modeling\n"
      ],
      "metadata": {
        "id": "FpA58RjL5rMG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "msBJUZE_5uId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n"
      ],
      "metadata": {
        "id": "dn_leZJn5zk2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AwXLzrr452d1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deployment"
      ],
      "metadata": {
        "id": "CW8MxReL52_f"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-csn0D_w54RN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}